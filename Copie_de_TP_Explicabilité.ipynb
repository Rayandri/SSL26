{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMoulla/SSL26/blob/main/Copie_de_TP_Explicabilit%C3%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3lrZMMsGRNQ"
      },
      "source": [
        "# **TP : Explicabilité des modèles de deep learning pour les images avec LIME et SHAP**\n",
        "\n",
        "## **Objectif**\n",
        "Ce TP vise à explorer comment interpréter les prédictions d’un modèle de deep learning appliqué aux images à l’aide de **LIME** et **SHAP**. Ces outils permettent d’identifier les parties de l’image qui influencent le plus les décisions du modèle, contribuant ainsi à rendre les modèles plus transparents et compréhensibles.\n",
        "\n",
        "---\n",
        "\n",
        "## **Contexte**\n",
        "Les modèles de deep learning, bien qu’efficaces, sont souvent considérés comme des boîtes noires. Comprendre pourquoi un modèle fait une certaine prédiction est essentiel pour :  \n",
        "1. Identifier les biais potentiels dans le modèle.  \n",
        "2. Valider la fiabilité des prédictions dans des applications sensibles.  \n",
        "3. Construire la confiance des utilisateurs finaux.\n",
        "\n",
        "Dans ce TP, nous allons :  \n",
        "1. Charger un modèle de classification d’images pré-entraîné avec PyTorch.  \n",
        "2. Utiliser ce modèle pour prédire les classes d’images fournies.  \n",
        "3. Appliquer **LIME** et **SHAP** pour expliquer ces prédictions.  \n",
        "4. Comparer les visualisations générées par ces deux outils et analyser les résultats.\n",
        "\n",
        "---\n",
        "\n",
        "## **Plan du TP**\n",
        "\n",
        "### **Étape 1 : Préparation de l’environnement**\n",
        "1. Installer les bibliothèques nécessaires pour PyTorch, LIME, et SHAP.  \n",
        "2. Télécharger ou préparer un ensemble d’images pour les tests (votre dataset préféré, des images issues d’ImageNet, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "### **Étape 2 : Charger un modèle pré-entraîné**\n",
        "1. Nous allons utiliser un modèle pré-entraîné disponible dans PyTorch, comme ResNet18 ou VGG16, avec des poids pré-entraînés sur ImageNet.  \n",
        "2. Le modèle sera utilisé pour effectuer des prédictions sur les images, après les avoir redimensionnées et normalisées en fonction des besoins du modèle.  \n",
        "3. Pour chaque image, nous afficherons les classes prédites avec leurs scores de confiance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Étape 3 : Explicabilité avec LIME**\n",
        "1. **Présentation de LIME :**  \n",
        "   LIME fonctionne en créant des perturbations localisées sur une image (par exemple, en masquant des zones spécifiques) pour mesurer l’impact de chaque zone sur la prédiction.  \n",
        "2. **Étapes :**  \n",
        "   - Segmenter l’image en pixels ou en superpixels.  \n",
        "   - Identifier les zones les plus influentes pour une prédiction donnée.  \n",
        "   - Générer une heatmap mettant en évidence les zones importantes pour la classe prédite.  \n",
        "3. Nous analyserons les résultats pour comprendre quelles parties de l’image influencent le plus la prédiction.\n",
        "\n",
        "---\n",
        "\n",
        "### **Étape 4 : Explicabilité avec SHAP**\n",
        "1. **Présentation de SHAP :**  \n",
        "   SHAP utilise la théorie des jeux pour attribuer une importance à chaque pixel ou groupe de pixels, en mesurant leur contribution à la prédiction.  \n",
        "2. **Étapes :**  \n",
        "   - Fournir les images prétraitées au modèle pour calculer les valeurs SHAP.  \n",
        "   - Générer une visualisation qui montre les contributions positives et négatives des pixels ou des superpixels pour la prédiction.  \n",
        "3. Nous interpréterons les visualisations en observant les zones qui favorisent ou défavorisent la classe prédite.\n",
        "\n",
        "---\n",
        "\n",
        "### **Étape 5 : Comparaison et analyse des résultats**\n",
        "1. **Comparaison des visualisations :**  \n",
        "   - Identifier les différences et similitudes entre les résultats de LIME et SHAP.  \n",
        "   - Analyser les zones mises en évidence par chaque méthode.  \n",
        "2. **Discussion :**  \n",
        "   - Quels sont les points forts et limites de chaque méthode ?  \n",
        "   - Les deux approches donnent-elles des explications cohérentes ?  \n",
        "   - Quels défis rencontrons-nous en utilisant ces outils avec des modèles de grande taille ou des images complexes ?\n",
        "\n",
        "---\n",
        "\n",
        "## **Livrables attendus**\n",
        "1. Les heatmaps générées par LIME et SHAP pour au moins deux images.  \n",
        "2. Une analyse comparative des résultats obtenus avec LIME et SHAP.  \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 1 : Préparation de l'environnement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 2 : Charger un modèle pré-entraîné (ResNet18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/rayandrissi/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 89.2MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded: ResNet18 with 1000 ImageNet classes\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "IMAGENET_URL = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
        "imagenet_classes = requests.get(IMAGENET_URL).text.strip().split(\"\\n\")\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "print(f\"Model loaded: ResNet18 with {len(imagenet_classes)} ImageNet classes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Charger des images de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "UnidentifiedImageError",
          "evalue": "cannot identify image file <_io.BytesIO object at 0x118880db0>",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mUnidentifiedImageError\u001b[39m                    Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m image_urls:\n\u001b[32m      8\u001b[39m     response = requests.get(url)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m     images.append(img)\n\u001b[32m     12\u001b[39m fig, axes = plt.subplots(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(images), figsize=(\u001b[32m12\u001b[39m, \u001b[32m5\u001b[39m))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dev/epita/majeur/cv-tp-rmoulla/SSL26/venv/lib/python3.11/site-packages/PIL/Image.py:3579\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3577\u001b[39m     warnings.warn(message)\n\u001b[32m   3578\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[32m-> \u001b[39m\u001b[32m3579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
            "\u001b[31mUnidentifiedImageError\u001b[39m: cannot identify image file <_io.BytesIO object at 0x118880db0>"
          ]
        }
      ],
      "source": [
        "image_urls = [\n",
        "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Cat_November_2010-1a.jpg/1200px-Cat_November_2010-1a.jpg\",\n",
        "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/YellowLabradorLooking_new.jpg/1200px-YellowLabradorLooking_new.jpg\",\n",
        "]\n",
        "\n",
        "images = []\n",
        "for url in image_urls:\n",
        "    response = requests.get(url)\n",
        "    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    images.append(img)\n",
        "\n",
        "fig, axes = plt.subplots(1, len(images), figsize=(12, 5))\n",
        "for i, img in enumerate(images):\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].axis(\"off\")\n",
        "    axes[i].set_title(f\"Image {i+1}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prédictions du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(img):\n",
        "    input_tensor = preprocess(img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "    top5_prob, top5_idx = torch.topk(probabilities, 5)\n",
        "    return [(imagenet_classes[idx], prob.item()) for idx, prob in zip(top5_idx, top5_prob)]\n",
        "\n",
        "for i, img in enumerate(images):\n",
        "    print(f\"\\n=== Image {i+1} - Top 5 prédictions ===\")\n",
        "    predictions = predict(img)\n",
        "    for class_name, prob in predictions:\n",
        "        print(f\"  {class_name}: {prob*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 3 : Explicabilité avec LIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "\n",
        "def batch_predict_lime(images_array):\n",
        "    batch = torch.stack([preprocess(Image.fromarray(img)) for img in images_array])\n",
        "    batch = batch.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(batch)\n",
        "    probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "    return probs.cpu().numpy()\n",
        "\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "lime_results = []\n",
        "for i, img in enumerate(images):\n",
        "    print(f\"Generating LIME explanation for image {i+1}...\")\n",
        "    img_array = np.array(img)\n",
        "    explanation = explainer.explain_instance(\n",
        "        img_array,\n",
        "        batch_predict_lime,\n",
        "        top_labels=5,\n",
        "        hide_color=0,\n",
        "        num_samples=1000\n",
        "    )\n",
        "    lime_results.append(explanation)\n",
        "    print(f\"  Done!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualisation LIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(len(images), 3, figsize=(15, 5*len(images)))\n",
        "\n",
        "for i, (img, explanation) in enumerate(zip(images, lime_results)):\n",
        "    top_label = explanation.top_labels[0]\n",
        "    \n",
        "    axes[i, 0].imshow(img)\n",
        "    axes[i, 0].set_title(f\"Image originale\\n{imagenet_classes[top_label]}\")\n",
        "    axes[i, 0].axis(\"off\")\n",
        "    \n",
        "    temp, mask = explanation.get_image_and_mask(\n",
        "        top_label, positive_only=True, num_features=5, hide_rest=False\n",
        "    )\n",
        "    axes[i, 1].imshow(mark_boundaries(temp/255.0, mask))\n",
        "    axes[i, 1].set_title(\"LIME - Zones positives\")\n",
        "    axes[i, 1].axis(\"off\")\n",
        "    \n",
        "    temp, mask = explanation.get_image_and_mask(\n",
        "        top_label, positive_only=False, num_features=10, hide_rest=False\n",
        "    )\n",
        "    axes[i, 2].imshow(mark_boundaries(temp/255.0, mask))\n",
        "    axes[i, 2].set_title(\"LIME - Toutes les zones\")\n",
        "    axes[i, 2].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 4 : Explicabilité avec SHAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "def model_predict_shap(x):\n",
        "    x = torch.tensor(x).permute(0, 3, 1, 2).float().to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(x)\n",
        "    return output.cpu().numpy()\n",
        "\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "preprocessed_images = []\n",
        "for img in images:\n",
        "    img_resized = img.resize((224, 224))\n",
        "    img_array = np.array(img_resized) / 255.0\n",
        "    img_normalized = (img_array - mean) / std\n",
        "    preprocessed_images.append(img_normalized)\n",
        "\n",
        "preprocessed_images = np.array(preprocessed_images)\n",
        "\n",
        "masker = shap.maskers.Image(\"inpaint_telea\", preprocessed_images[0].shape)\n",
        "explainer_shap = shap.Explainer(model_predict_shap, masker, output_names=imagenet_classes)\n",
        "\n",
        "print(\"Generating SHAP explanations (this may take a few minutes)...\")\n",
        "shap_values = explainer_shap(preprocessed_images, max_evals=500, batch_size=50)\n",
        "print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualisation SHAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shap.image_plot(shap_values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 5 : Comparaison LIME vs SHAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(len(images), 3, figsize=(15, 5*len(images)))\n",
        "\n",
        "for i, (img, lime_exp) in enumerate(zip(images, lime_results)):\n",
        "    top_label = lime_exp.top_labels[0]\n",
        "    \n",
        "    axes[i, 0].imshow(img)\n",
        "    axes[i, 0].set_title(f\"Original\\n{imagenet_classes[top_label]}\")\n",
        "    axes[i, 0].axis(\"off\")\n",
        "    \n",
        "    temp, mask = lime_exp.get_image_and_mask(\n",
        "        top_label, positive_only=True, num_features=5, hide_rest=False\n",
        "    )\n",
        "    axes[i, 1].imshow(mark_boundaries(temp/255.0, mask))\n",
        "    axes[i, 1].set_title(\"LIME\")\n",
        "    axes[i, 1].axis(\"off\")\n",
        "    \n",
        "    shap_img = shap_values[i].values[:, :, :, top_label]\n",
        "    shap_img_norm = (shap_img - shap_img.min()) / (shap_img.max() - shap_img.min() + 1e-8)\n",
        "    axes[i, 2].imshow(np.array(img.resize((224, 224))))\n",
        "    axes[i, 2].imshow(shap_img_norm.sum(axis=-1), cmap='jet', alpha=0.5)\n",
        "    axes[i, 2].set_title(\"SHAP\")\n",
        "    axes[i, 2].axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"Comparaison LIME vs SHAP\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNNBOZYlnEOn4VbJpHCAx3z",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
