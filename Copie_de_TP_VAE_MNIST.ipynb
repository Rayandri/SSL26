{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMoulla/SSL26/blob/main/Copie_de_TP_VAE_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7Q8uwOq0X-G"
      },
      "source": [
        "# **Variational Autoencoder (VAE) with Latent Space Interpolation on MNIST**\n",
        "\n",
        "## Project Description\n",
        "\n",
        "In this practical assignment, we explore the concepts of **Variational Autoencoders (VAEs)** by implementing and training a convolutional VAE on the MNIST dataset. The primary objectives are to learn how VAEs encode data into a structured latent space and to investigate how this latent space can be leveraged for generating new data and understanding data features.\n",
        "\n",
        "### Key Learning Goals:\n",
        "1. **Data Loading and Preprocessing**: Load the MNIST dataset and prepare it for training a convolutional neural network.\n",
        "2. **VAE Architecture**: Define a VAE model with convolutional layers for encoding and decoding images, including a reparameterization trick to ensure smooth sampling from the latent space.\n",
        "3. **VAE Loss Function**: Understand and implement the VAE loss, which combines reconstruction loss and KL-divergence to balance accurate reconstructions with a regularized latent space.\n",
        "4. **Model Training**: Train the VAE model on MNIST, observe loss trends, and understand the impact of balancing reconstruction quality and latent space regularization.\n",
        "5. **Latent Space Visualization**: Perform latent space interpolation by linearly blending between two points in the latent space, generating smooth transitions between two digit classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfrpIiPJx7nG"
      },
      "source": [
        "## Step 1: Loading the MNIST Dataset\n",
        "\n",
        "In this first step, we load the MNIST dataset, which contains 28x28 grayscale images of handwritten digits (0-9). Each image represents a single digit.\n",
        "\n",
        "We use PyTorch's `torchvision.datasets` and `DataLoader` to load and preprocess the data efficiently.\n",
        "\n",
        "### Code Breakdown:\n",
        "- **Transforms**: `transforms.Compose([transforms.ToTensor()])` converts each image into a tensor format. This allows the data to be used in PyTorch models.\n",
        "- **Dataset**: `datasets.MNIST` loads the MNIST dataset and applies the specified transform. We set `download=True` to ensure the data is downloaded if it hasn't been already.\n",
        "- **DataLoader**: The `DataLoader` wraps the dataset and enables batching, shuffling, and parallel loading.\n",
        "\n",
        "This setup provides a `train_loader`, which we will use to feed batches of images into our model during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "liyE44fBkzwN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Loading MNIST data\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68vTRMlqyLte"
      },
      "source": [
        "## Step 2: Defining the Convolutional VAE Model\n",
        "\n",
        "In this section, we define the architecture of a **Variational Autoencoder (VAE)** with convolutional layers, designed to process the MNIST dataset.\n",
        "\n",
        "### Code Breakdown:\n",
        "\n",
        "1. **Initialization and Latent Dimension**:\n",
        "   - `latent_dim=2` is defined as the dimensionality of the latent space. Setting it to a low number (e.g., 2) makes the latent space easy to visualize.\n",
        "\n",
        "2. **Encoder**:\n",
        "   - The encoder consists of three convolutional layers:\n",
        "     - The first layer transforms the input image of size 1x28x28 to a feature map of size 32x14x14.\n",
        "     - The second layer further reduces it to a size of 64x7x7.\n",
        "     - The third layer compresses this to a size of 128x1x1.\n",
        "   - This results in a flattened feature vector, which is then fed into two fully connected layers (`fc_mu` and `fc_logvar`) to produce the **mean** (`mu`) and **log-variance** (`logvar`) of the latent space.\n",
        "\n",
        "3. **Reparameterization Trick**:\n",
        "   - In `reparameterize`, we use the mean and log-variance vectors to sample from a Gaussian distribution. This is done by computing:\n",
        "\n",
        "     z = \\mu + \\sigma \\cdot \\epsilon\n",
        "\n",
        "   - Here, `epsilon` is a random noise sampled from a standard normal distribution. This allows gradients to backpropagate through the sampling process.\n",
        "\n",
        "4. **Decoder**:\n",
        "   - The decoder starts with a fully connected layer to expand the latent vector (`z`) back to a shape compatible with the convolutional layers.\n",
        "   - It then passes through three transposed convolutional layers to reconstruct the original image shape:\n",
        "     - The first layer reshapes it to 64x7x7.\n",
        "     - The second layer outputs 32x14x14.\n",
        "     - The final layer produces the original shape, 1x28x28, with pixel values normalized between 0 and 1 using a Sigmoid activation.\n",
        "\n",
        "5. **Forward Pass**:\n",
        "   - `encode`: Passes the input through the encoder to obtain `mu` and `logvar`.\n",
        "   - `reparameterize`: Samples from the latent distribution using `mu` and `logvar`.\n",
        "   - `decode`: Reconstructs the image from the latent vector `z`.\n",
        "   - Returns the reconstructed image along with `mu` and `logvar` for further use in the loss calculation.\n",
        "\n",
        "This architecture allows the VAE to encode images to a low-dimensional latent space and then decode them back to their original shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bNSNUz9kktWX"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvVAE(nn.Module):\n",
        "    def __init__(self, latent_dim=2):\n",
        "        super(ConvVAE, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 7),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc_mu = nn.Linear(128, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
        "\n",
        "        self.fc_decode = nn.Linear(latent_dim, 128)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 7),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, 4, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(h.size(0), -1)\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = self.fc_decode(z)\n",
        "        h = h.view(h.size(0), 128, 1, 1)\n",
        "        return self.decoder(h)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXxPhUHey9fb"
      },
      "source": [
        "## Step 3: Defining the VAE Loss Function\n",
        "\n",
        "The Variational Autoencoder (VAE) loss function combines two key components: **Reconstruction Loss** and **KL-Divergence Loss**. Together, these terms encourage the VAE to produce high-quality reconstructions while also regularizing the latent space.\n",
        "\n",
        "### Code Breakdown:\n",
        "\n",
        "1. **Flattening**:\n",
        "   - Both `recon_x` (the reconstructed image) and `x` (the original image) are flattened to a shape of `[batch_size, 784]` to match the expected input shape for the binary cross-entropy function.\n",
        "\n",
        "2. **Reconstruction Loss (Binary Cross-Entropy)**:\n",
        "   - We use Binary Cross-Entropy (BCE) as the reconstruction loss. This term measures the pixel-wise difference between the original and reconstructed images.\n",
        "   - `BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')` sums the error over all pixels in the batch, promoting accurate reconstructions.\n",
        "\n",
        "3. **KL-Divergence Loss**:\n",
        "   - The KL-Divergence term measures the difference between the learned latent distribution `q(z|x)` and a standard normal distribution `p(z) = N(0, 1)`.\n",
        "   - The calculation:\n",
        "\\[\n",
        "     KLD = -0.5 \\sum (1 + \\log(\\sigma^2) - \\mu^2 - \\sigma^2)\n",
        "\\]\n",
        "   - This term penalizes deviations from the standard normal distribution, helping to structure the latent space and ensure continuity.\n",
        "\n",
        "4. **Total VAE Loss**:\n",
        "   - The final loss is the sum of the reconstruction loss (BCE) and the KL-Divergence loss (KLD). Minimizing this total loss encourages the model to reconstruct images accurately while keeping the latent space organized.\n",
        "\n",
        "By using this combined loss, the VAE learns to generate images that resemble the input data and maintain a well-structured latent space that facilitates tasks like image generation and interpolation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rmNTJwtSlMEu"
      },
      "outputs": [],
      "source": [
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    recon_x = recon_x.view(-1, 784)\n",
        "    x = x.view(-1, 784)\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH_iNES9zVHS"
      },
      "source": [
        "## Step 4: Training the VAE Model\n",
        "\n",
        "In this step, we set up and execute the training loop for the Variational Autoencoder (VAE) model. The training process optimizes the model’s parameters to minimize the combined loss function over multiple epochs.\n",
        "\n",
        "### Code Breakdown:\n",
        "\n",
        "1. **Setting up the Device**:\n",
        "   - The code detects if a GPU is available using `torch.cuda.is_available()`. If so, it uses the GPU for faster training; otherwise, it defaults to the CPU.\n",
        "\n",
        "2. **Model and Optimizer Initialization**:\n",
        "   - We create an instance of the `ConvVAE` model with a specified `latent_dim`. A small latent dimension (like 2) makes it easier to visualize the latent space later.\n",
        "   - The model is sent to the chosen device (CPU or GPU).\n",
        "   - The optimizer is set up using Adam with a learning rate of 0.001, which is well-suited for training VAEs.\n",
        "\n",
        "3. **Training Loop**:\n",
        "   - `num_epochs` defines the number of times the entire dataset is processed during training.\n",
        "   - For each epoch:\n",
        "     - The model is set to training mode (`model.train()`), which activates features like dropout (if used).\n",
        "     - `train_loss` is initialized to accumulate the total loss over the epoch.\n",
        "     - For each batch in `train_loader`:\n",
        "       - **Data Transfer**: The batch of images is transferred to the chosen device.\n",
        "       - **Gradient Reset**: `optimizer.zero_grad()` resets gradients from the previous batch to prevent accumulation.\n",
        "       - **Forward Pass**: The data is passed through the model, which outputs `recon_batch` (reconstructed images), `mu` (mean), and `logvar` (log-variance) of the latent distribution.\n",
        "       - **Loss Calculation**: `loss_function` computes the VAE loss by combining reconstruction and KL-divergence losses.\n",
        "       - **Backpropagation**: `loss.backward()` computes the gradients for all model parameters.\n",
        "       - **Optimization Step**: `optimizer.step()` updates the parameters using the calculated gradients.\n",
        "       - The batch loss is added to `train_loss` to track the total loss for the epoch.\n",
        "   \n",
        "   - After each epoch, the average loss for that epoch is printed for monitoring progress. This average loss helps to assess if the model is converging.\n",
        "\n",
        "The training loop fine-tunes the VAE’s parameters to generate accurate reconstructions and a well-structured latent space, which can be visualized or used for generative tasks after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Zoea4byXlOo4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "Epoch 1/10, Loss: 188.5525\n",
            "Epoch 2/10, Loss: 162.0786\n",
            "Epoch 3/10, Loss: 157.5353\n",
            "Epoch 4/10, Loss: 155.1990\n",
            "Epoch 5/10, Loss: 153.5922\n",
            "Epoch 6/10, Loss: 152.4239\n",
            "Epoch 7/10, Loss: 151.5749\n",
            "Epoch 8/10, Loss: 150.8069\n",
            "Epoch 9/10, Loss: 150.1274\n",
            "Epoch 10/10, Loss: 149.6505\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "latent_dim = 2\n",
        "model = ConvVAE(latent_dim=latent_dim).to(device)\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss / len(train_loader.dataset):.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8HSV3JuzqD3"
      },
      "source": [
        "## Step 5: Interpolation in the Latent Space\n",
        "\n",
        "In this final step, we perform **latent space interpolation** to explore the continuity of the VAE's learned latent space. By interpolating between two points in the latent space (representing different digits), we can generate a smooth transition between them.\n",
        "\n",
        "### Code Breakdown:\n",
        "\n",
        "1. **Defining the Interpolation Function**:\n",
        "   - `interpolate_and_generate`: This function takes two latent vectors (`z_start` and `z_end`) and generates intermediate points between them.\n",
        "   - **Interpolation**: We create a series of interpolated points by linearly blending `z_start` and `z_end` with interpolation weights `t` ranging from 0 to 1.\n",
        "   - **Decoding**: Each interpolated vector is decoded by the VAE’s decoder to produce an image, which is stored in the `images` list.\n",
        "\n",
        "2. **Selecting Points to Interpolate**:\n",
        "   - We randomly select two samples from the dataset (e.g., images of \"1\" and \"7\").\n",
        "   - These images are passed through the encoder to obtain their latent representations, `z_start` and `z_end`.\n",
        "\n",
        "3. **Generating and Visualizing the Interpolation**:\n",
        "   - The interpolated latent points are decoded back into images.\n",
        "   - We then plot each decoded image side-by-side to visualize the transformation from the starting digit to the ending digit.\n",
        "   \n",
        "   Each intermediate image represents a gradual change in the latent space between `z_start` and `z_end`, showing how the model \"morphs\" one digit into another.\n",
        "\n",
        "### Visualization Explanation:\n",
        "\n",
        "The visualization showcases the VAE’s ability to generate new images by sampling from the latent space. The smooth transition between digits demonstrates that the VAE has learned a well-structured, continuous latent space where similar concepts (like digits) are located close to each other. This capability can be useful for generating synthetic data or exploring variations in features.\n",
        "\n",
        "By interpolating in this manner, we can observe how the VAE understands and generates the fundamental features of the digits in the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "seg0fyNyntjh"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x300 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def interpolate_and_generate(model, z_start, z_end, steps=10):\n",
        "    images = []\n",
        "    for t in torch.linspace(0, 1, steps):\n",
        "        z = (1 - t) * z_start + t * z_end\n",
        "        decoded = model.decode(z)\n",
        "        img = decoded.squeeze().cpu().numpy()\n",
        "        images.append(img)\n",
        "    return images\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    sample1 = train_dataset[0][0].unsqueeze(0).to(device)\n",
        "    sample2 = train_dataset[1][0].unsqueeze(0).to(device)\n",
        "    \n",
        "    mu1, _ = model.encode(sample1)\n",
        "    mu2, _ = model.encode(sample2)\n",
        "    \n",
        "    images = interpolate_and_generate(model, mu1, mu2, steps=10)\n",
        "\n",
        "fig, axes = plt.subplots(1, len(images), figsize=(15, 3))\n",
        "for i, img in enumerate(images):\n",
        "    axes[i].imshow(img, cmap=\"gray\")\n",
        "    axes[i].axis(\"off\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOiHIOmEBKNtLyYoYbwRmzg",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
